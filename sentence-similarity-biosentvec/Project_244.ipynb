{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Driven Clinical Decision Support: Enhancing Disease Diagnosis Exploiting Patients Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides the sentence similarity and word similarity scores using BioSentVec and BioWordVec models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download pre-trained BioSentVec model and install all the related python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sent2vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary python libraries to access the Universal Sentence Encoder Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load BioSentVec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model successfully loaded\n"
     ]
    }
   ],
   "source": [
    "model_path = '../test_data/BioSentVec_PubMed_MIMICIII-bigram_d700.bin'\n",
    "model = sent2vec.Sent2vecModel()\n",
    "try:\n",
    "    model.load_model(model_path)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print('model successfully loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Universal Sentence Encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the standard version of the universal sentence encoder and is only able to process the english language. Other versions offer a broader language variety and are suited for different forms of textual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "universal_sentence_encoder = hub.load(module_url)\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akhil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_sentence(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of using the preprocess_sentence function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast cancers her2 amplification higher risk cns metastasis poorer prognosis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akhil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentence = preprocess_sentence('Breast cancers with HER2 amplification have a higher risk of CNS metastasis and poorer prognosis.')\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retrieve a sentence vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. BioSentVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a sentence is preprocessed, we can pass it to the BioSentVec model to retrieve a vector representation of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.27253592  0.04016513 -0.13868049  0.06607066  0.03410426  0.03702081\n",
      "   0.04780459  0.318374    0.1389506   0.14894584  0.03802885  0.16076139\n",
      "   0.27367333  0.28947747 -0.3635127   0.1523829   0.00113982  0.15947492\n",
      "  -0.00115095 -0.3911827   0.06040372 -0.30060792  0.5700456  -0.3073153\n",
      "   0.05641874 -0.38538572  0.03242918 -0.01758919 -0.53824794 -0.2036874\n",
      "   0.09088504  0.42208442  0.01777515  0.26457042  0.00444555 -0.4244185\n",
      "   0.08552625 -0.01220523 -0.52954006 -0.19729511  0.3146897   0.39812556\n",
      "  -0.73728865 -0.15572241  0.12493155 -0.189124    0.30150056 -0.13335498\n",
      "  -0.22929646  0.1923776  -0.25276372  0.48184827 -0.11678692  0.074292\n",
      "  -0.3565283   0.06902904 -0.16303737 -0.1516651  -0.16457589  0.2640424\n",
      "  -0.2330729   0.03231101  0.3361209   0.35289383 -0.23463576 -0.29648\n",
      "  -0.3083266   0.39252853 -0.24566592 -0.2444962   0.20645703 -0.04719147\n",
      "   0.10580424  0.00649089 -0.2572806  -0.333023   -0.03018534 -0.042082\n",
      "  -0.03446042  0.1267659   0.37817308 -0.38865507 -0.20552012 -0.34621498\n",
      "  -0.1216602  -0.04652812 -0.02347284 -0.24400087  0.16549529 -0.06411781\n",
      "   0.01422617 -0.12668294  0.5960534   0.02109158  0.16629732  0.17482263\n",
      "  -0.12253477  0.12936321  0.15015826  0.09612935  0.03910794 -0.09146566\n",
      "  -0.43439966 -0.07247142  0.26412925 -0.17527688 -0.13276757  0.20395164\n",
      "  -0.05921361  0.16484062  0.18909335 -0.09065875  0.14640309 -0.04357425\n",
      "  -0.31174526  0.13512115 -0.15826614  0.05530081 -0.32504323  0.20767705\n",
      "  -0.0941015  -0.01312271 -0.10174442  0.03745251 -0.14577436 -0.11705701\n",
      "   0.24673483 -0.29994592 -0.03089786  0.05221201  0.4669998   0.00442661\n",
      "   0.26255304 -0.0520683   0.24765283 -0.28208813  0.02101091 -0.2309345\n",
      "  -0.27185237 -0.06502334  0.04894404 -0.12051236  0.19054177  0.33650944\n",
      "   0.21696663 -0.09382363  0.04122993  0.02740302 -0.1523489   0.06898607\n",
      "   0.3646409   0.00145511  0.23472148  0.08020525 -0.06348559  0.38403732\n",
      "  -0.19614884  0.05751961  0.25427777 -0.06352087 -0.00501605  0.12870164\n",
      "  -0.12095109  0.01321181  0.30547306 -0.12648751 -0.22209032  0.20374553\n",
      "   0.30825672 -0.2554286   0.08377017  0.29781944 -0.29899627 -0.07965458\n",
      "  -0.18984668 -0.13143328  0.02504029 -0.15191814 -0.02388869 -0.06577343\n",
      "  -0.3067318  -0.07350196  0.07532682  0.04892201 -0.01745802 -0.10110494\n",
      "  -0.01072426 -0.01015269  0.232459   -0.32037356 -0.09599126 -0.09170408\n",
      "   0.07457744  0.10797482 -0.16640112 -0.02322571 -0.22640668 -0.32701987\n",
      "   0.29183394 -0.08617553  0.07842809  0.18027404  0.32083857 -0.29391503\n",
      "  -0.37091807  0.1317559  -0.04133325  0.02105725  0.06929144 -0.45651826\n",
      "   0.06086781  0.43626273  0.02961666  0.14899905  0.25753006 -0.15902318\n",
      "   0.27228698 -0.012717   -0.3683212   0.3146828   0.00246162 -0.2528494\n",
      "  -0.01773234 -0.16845916  0.15252198  0.05695811 -0.4019833   0.11004736\n",
      "  -0.4061533  -0.01654322 -0.08055133 -0.06888298  0.03975208 -0.12505263\n",
      "  -0.4766509  -0.1302982  -0.15458837  0.19499418 -0.20499982  0.01576013\n",
      "  -0.04100087 -0.03823095  0.01355971  0.31886473  0.3207466   0.30761683\n",
      "  -0.58859974 -0.05454841  0.09202857  0.07083365  0.12124014 -0.15489404\n",
      "  -0.1249956   0.19807188  0.02977463  0.06490497  0.0862143   0.09217765\n",
      "  -0.39212793 -0.1090506   0.3700054   0.2391053   0.1204542   0.06182214\n",
      "  -0.20115142 -0.19802506 -0.12779813  0.18747202  0.0733431  -0.09663613\n",
      "  -0.24403886  0.16471218 -0.12632118  0.3087525  -0.12539518 -0.02084013\n",
      "  -0.07293235 -0.38778466 -0.20683263  0.06490733  0.05344631 -0.28166145\n",
      "   0.0709516  -0.05099153  0.26141232 -0.02879013  0.3863618  -0.03771535\n",
      "   0.04465518  0.25189495 -0.05824171  0.04616535 -0.33440518  0.05650642\n",
      "   0.01963214  0.04899212 -0.12409336 -0.02178784 -0.02102915  0.02570555\n",
      "   0.13620213  0.01591191 -0.51012826 -0.11808088  0.16109395 -0.12763613\n",
      "  -0.09608371 -0.223153    0.10025517  0.110238    0.04289898  0.43777797\n",
      "  -0.07757877  0.3245564  -0.0072146   0.36475793 -0.23756203 -0.14881566\n",
      "   0.1897787  -0.22575381  0.32615083  0.16910845 -0.08788409 -0.07606266\n",
      "  -0.03706334  0.08212929 -0.19536538  0.19984807  0.04603511 -0.26996538\n",
      "   0.04950259 -0.03615545  0.1406415   0.2947527  -0.00611998 -0.05985891\n",
      "   0.01984618 -0.03949784 -0.01525426  0.29419264 -0.01415043 -0.17652188\n",
      "  -0.06262738 -0.22616321  0.25551927 -0.02472711  0.15726517 -0.14524549\n",
      "  -0.11207764  0.10489892  0.14721154  0.1193269  -0.0470333   0.08068092\n",
      "   0.06711143 -0.1101417   0.00740551  0.23555118 -0.04884436 -0.29348636\n",
      "   0.36853147  0.09429416  0.22065276  0.23430087 -0.0068337   0.06033167\n",
      "   0.14368132 -0.28589955  0.32065156 -0.02703334  0.14414166  0.11144061\n",
      "  -0.09757377  0.08389441  0.4110573  -0.17193225 -0.17498371  0.12369279\n",
      "  -0.17010431 -0.09807961 -0.07679521  0.13369125  0.13676417 -0.16726981\n",
      "   0.39855367  0.0587613   0.12028298  0.01342451 -0.07659346  0.03576399\n",
      "  -0.04420809  0.12297461  0.02851038 -0.01444774 -0.01379851 -0.08932398\n",
      "   0.28293097 -0.1373159   0.16300136  0.12364378 -0.2913006   0.25817928\n",
      "  -0.01344534 -0.24683551 -0.08785618 -0.1017781  -0.12594536 -0.17217784\n",
      "   0.12956655 -0.13296415  0.22922768  0.15616998 -0.2765172  -0.3030905\n",
      "   0.03086687 -0.00273167 -0.15588386 -0.05675261 -0.09152196  0.26230586\n",
      "  -0.01163875 -0.2478254   0.260964   -0.05098752 -0.02663371  0.08234623\n",
      "   0.34928283  0.8313451   0.02071937  0.24742903 -0.06239458  0.09169593\n",
      "   0.00140471 -0.06047087 -0.35359547  0.12234055  0.18345007 -0.14262569\n",
      "  -0.11202564  0.274945   -0.06307555  0.20897087  0.22961979 -0.31827667\n",
      "   0.12434521  0.09456863 -0.132533   -0.13584521 -0.36066884 -0.05460902\n",
      "  -0.14705043 -0.08507536  0.22685164  0.24383776  0.20274445  0.07966835\n",
      "  -0.06932851 -0.01657332 -0.35544744 -0.22558543 -0.0651169   0.08119379\n",
      "   0.3001279  -0.01761239 -0.01498686  0.11016284  0.2519153   0.02833793\n",
      "  -0.28951043  0.06437117 -0.25671995 -0.03743215 -0.22699313  0.24525918\n",
      "   0.04435244 -0.25781178  0.00997334 -0.07835439  0.22938563 -0.07016336\n",
      "  -0.24928015 -0.18942201 -0.1236209  -0.44305456  0.53566355 -0.18446858\n",
      "   0.30429277 -0.11268931 -0.11295509  0.25952902  0.19171143  0.07295282\n",
      "  -0.01309466  0.15677398 -0.1115496  -0.11746953 -0.34486744  0.01961437\n",
      "   0.08887484  0.1231166  -0.22707342  0.14050385  0.02042234 -0.27477872\n",
      "  -0.32859874  0.15609217 -0.15527791 -0.03412036 -0.13152814  0.5236449\n",
      "   0.19360445 -0.18125863  0.41408825  0.17874481 -0.0879835  -0.11195815\n",
      "  -0.08948261  0.23711275  0.10845808 -0.22963704  0.02916685  0.04244966\n",
      "  -0.0449315  -0.16313884 -0.36450905  0.06882233  0.10855233  0.3169161\n",
      "  -0.33788228 -0.11677711 -0.36983833  0.09579375 -0.02219467  0.11477247\n",
      "   0.02546611  0.08161401  0.08159067 -0.2501985  -0.23828559  0.36675447\n",
      "  -0.15668799  0.20695254  0.27773544  0.47669446  0.01058489  0.27333802\n",
      "  -0.39817995  0.23312205  0.11152606 -0.15429601 -0.21768859  0.02197697\n",
      "  -0.05461999  0.11158564 -0.3009951   0.04721674  0.33778647 -0.22506985\n",
      "   0.2090023   0.13018404 -0.17677754 -0.09073435 -0.157161   -0.12982582\n",
      "  -0.13903137  0.01262058 -0.06162163  0.11507569  0.21850152  0.09291503\n",
      "   0.13182876  0.02859347  0.12657352  0.3068309  -0.15490891 -0.04232102\n",
      "   0.062854   -0.15683283 -0.2431332  -0.20136073 -0.32315066  0.05642203\n",
      "  -0.16685694  0.24037287 -0.10076776 -0.15987408  0.04036417  0.06853651\n",
      "   0.06721435  0.09657718  0.21487527  0.04389333 -0.42330703 -0.12825093\n",
      "  -0.12326848 -0.26695827  0.0649719  -0.32621393 -0.09277593  0.04695158\n",
      "   0.16902225  0.12192411  0.02212488 -0.13833636  0.21684082 -0.15384167\n",
      "   0.00954215  0.21829392 -0.10491441  0.38043278 -0.08237162  0.22160071\n",
      "   0.07220576  0.3385922   0.18430929 -0.01216795  0.20997563  0.04614374\n",
      "   0.5460487  -0.02897776  0.14775318  0.31089064  0.27132967 -0.08209523\n",
      "   0.23873891 -0.06413503 -0.07715333 -0.02231805 -0.00694238  0.37205717\n",
      "  -0.1450972  -0.0704605  -0.02053621  0.11540693 -0.11201832 -0.1471214\n",
      "   0.04950135 -0.04224805  0.21448477 -0.22363718  0.02988946  0.07961679\n",
      "   0.02574715 -0.17271668  0.325553    0.01628166 -0.05568108 -0.3240605\n",
      "  -0.1429462   0.05608758 -0.01153869  0.03438982  0.08489512 -0.03345412\n",
      "  -0.04629951 -0.40246782  0.06087665  0.20731504 -0.20592833  0.2631903\n",
      "   0.12083606  0.03901361  0.22229938 -0.2662993   0.20107882 -0.20194705\n",
      "   0.12862273 -0.14036344 -0.23233241 -0.08034117  0.12506847 -0.1897902\n",
      "   0.0618707   0.15091741 -0.4029728  -0.10979341 -0.10763265  0.235283\n",
      "  -0.08089121  0.03753055  0.2415903  -0.33070192  0.03716518  0.33133337\n",
      "  -0.13763449 -0.0574756   0.32341847  0.10362037  0.12447642 -0.19017035\n",
      "   0.00549802  0.10385241  0.01570529 -0.11430962 -0.01734808 -0.10625661\n",
      "  -0.1896727   0.0568063   0.04407496  0.16548488]]\n"
     ]
    }
   ],
   "source": [
    "sentence_vector = model.embed_sentence(sentence)\n",
    "print(sentence_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can also use embed_sentences to retrieve vector representations of multiple sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the vector representation depends on the dimension parameter. In this case, we set the dimension to 700: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 700)\n"
     ]
    }
   ],
   "source": [
    "print(sentence_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a sentence is preprocessed, we can pass it to the Universal Sentence Encoder model to retrieve a vector representation of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.01706054 -0.02982263 -0.06450873 -0.00945025 -0.03946786 -0.077258\n",
      "  -0.04527489 -0.06925316 -0.00476587  0.05182612  0.08069605 -0.05177524\n",
      "  -0.04592242 -0.04435806  0.01292375 -0.04330221 -0.08106487 -0.03709086\n",
      "   0.06627572 -0.03122658  0.01539265  0.07002871  0.02392466 -0.01713471\n",
      "   0.0310583  -0.03737702  0.027232   -0.02219061 -0.0124025   0.07148183\n",
      "   0.05177425  0.08108076 -0.00427653 -0.07375586 -0.05576143  0.01741701\n",
      "  -0.04468859  0.06967676 -0.04987274 -0.05859317  0.03158952 -0.02929872\n",
      "   0.0215015  -0.02629163  0.04192941 -0.04903167 -0.05458231 -0.04037523\n",
      "  -0.06134297  0.06647678 -0.04487872 -0.04345295 -0.06080464 -0.04348946\n",
      "  -0.05720161 -0.0214076  -0.03062622  0.01864022  0.0283277   0.01318153\n",
      "  -0.05516872 -0.06180546  0.03157061 -0.03427697 -0.06046344 -0.01410388\n",
      "   0.02567874  0.03303296  0.06351852  0.0652984   0.02503775  0.06748096\n",
      "   0.00462156  0.04772187  0.0510456   0.02751398 -0.01434254  0.07380927\n",
      "   0.00462101 -0.05693014  0.07558877  0.07957336  0.06280757  0.01618009\n",
      "  -0.03963953  0.00983561  0.02604412 -0.02842521  0.02102662  0.01432356\n",
      "  -0.03796684  0.07071095  0.00805746  0.03495389  0.04285901  0.01219695\n",
      "  -0.0011974   0.05340707  0.03411036  0.04475879 -0.00368645  0.03083525\n",
      "   0.06678646 -0.01976889  0.0658456   0.00221735  0.00306581 -0.05956263\n",
      "   0.0010188   0.06516935 -0.07937347 -0.05739776  0.02673264  0.04284898\n",
      "  -0.00968548 -0.01743378 -0.0504784   0.01019778  0.05731171 -0.03684001\n",
      "  -0.06948613  0.05580418  0.00061519  0.02547961  0.03485722 -0.0397641\n",
      "  -0.05568918  0.03532877 -0.04141847 -0.05073671  0.01485707  0.08105876\n",
      "   0.04481429 -0.03248807 -0.01669317  0.03493425  0.05778471  0.07077385\n",
      "   0.03904844  0.02255414  0.06563922  0.04097562 -0.03475442 -0.02385972\n",
      "   0.02062055  0.02780036 -0.00716556 -0.0795014  -0.05367013  0.02893638\n",
      "   0.06852588 -0.01245587 -0.02475527 -0.06092625  0.0158423   0.02055498\n",
      "  -0.06865124 -0.04911914 -0.06675656 -0.0538724  -0.04096954  0.06000466\n",
      "  -0.05828339 -0.00365399  0.00391731 -0.07709993 -0.07388635  0.05102379\n",
      "   0.01949502 -0.05813583 -0.00148202 -0.08051783 -0.06785438  0.07001327\n",
      "   0.06746506 -0.0494054  -0.02439286 -0.03006406 -0.026718    0.03244586\n",
      "   0.03671165 -0.0165214  -0.04493577  0.04295131  0.06081214  0.02695449\n",
      "   0.06942286 -0.03379847 -0.03398345 -0.0723663   0.04744759 -0.04726491\n",
      "  -0.01711409  0.0136638   0.02234694  0.04608665  0.04815384 -0.01104325\n",
      "   0.02168086 -0.03241917 -0.02221176  0.04879502  0.06040967  0.03091732\n",
      "  -0.05210437  0.05603475 -0.05194407 -0.05271475 -0.03349389  0.03965942\n",
      "   0.05305358  0.01849852 -0.00412451 -0.03729869 -0.06587534  0.00751315\n",
      "   0.06674613  0.02230419 -0.0474311   0.06454217  0.03624735 -0.04641006\n",
      "   0.04679414  0.04101348 -0.01047366  0.05295023  0.0130115  -0.05019052\n",
      "  -0.04472562  0.05335503  0.03719546 -0.03793568 -0.00392208  0.04080231\n",
      "   0.01659797  0.03933737 -0.01563497  0.0525137   0.00568343 -0.01604218\n",
      "   0.012394   -0.04794568 -0.07756712  0.02057162  0.04740968  0.05463218\n",
      "  -0.0412036   0.05339271 -0.01242667 -0.07147543  0.05985804  0.03807913\n",
      "   0.02669032 -0.0642179  -0.00044721  0.03687155 -0.05137171  0.01684663\n",
      "  -0.05259082 -0.04366008  0.00187017 -0.03099283 -0.04578206  0.07071716\n",
      "  -0.04065597 -0.02268871 -0.01180075 -0.03539972 -0.06847797 -0.01248511\n",
      "  -0.05596287 -0.03235714  0.0474134   0.05286312 -0.06056814 -0.03532586\n",
      "   0.02094376 -0.03061981  0.00730366 -0.05177819 -0.0293935   0.05779368\n",
      "  -0.02360277 -0.00813117 -0.04635155 -0.02124056 -0.02579599 -0.02335327\n",
      "   0.01207552  0.04445786  0.07771128  0.0128588   0.00327976 -0.00514602\n",
      "  -0.03307199 -0.05580048  0.01127325  0.00800992 -0.01970835  0.01266492\n",
      "  -0.01486481  0.01470091  0.01068866  0.04788509 -0.07398974 -0.05278783\n",
      "  -0.05216161 -0.04771575  0.01909809  0.01295072  0.02847923 -0.01342636\n",
      "  -0.06500753 -0.02899458 -0.01669165 -0.07355104 -0.00209347  0.00441023\n",
      "  -0.05432557 -0.00650779 -0.03600447  0.00272223  0.03259845 -0.02744802\n",
      "   0.03263266  0.07950383 -0.06786795 -0.01886913 -0.0589062   0.00020725\n",
      "   0.0345258   0.04133177  0.00845711  0.02091471  0.05939933  0.00790352\n",
      "  -0.05228874  0.03750809 -0.00970996 -0.0416332   0.05570126 -0.02001455\n",
      "   0.06244738 -0.04040646  0.01222035  0.04565304 -0.05889083 -0.02573834\n",
      "   0.00702647  0.00826937 -0.01975815  0.05543774 -0.0150815   0.00152189\n",
      "  -0.00579688  0.02426102 -0.06849482  0.0065862  -0.00959657  0.04641501\n",
      "   0.05630873 -0.04543037 -0.0545543   0.02237976  0.03502543  0.03658181\n",
      "   0.03766841  0.01120555 -0.07801164 -0.02618901  0.02968883  0.02198969\n",
      "   0.07438018 -0.07290717  0.05835711 -0.04073805 -0.0453577  -0.05361401\n",
      "   0.02515254 -0.07141205 -0.07736989 -0.07691745  0.04752002 -0.04073824\n",
      "  -0.00405376  0.04521107  0.07464986 -0.03805187  0.06355393 -0.06085183\n",
      "  -0.03914654  0.0265358  -0.04585753  0.05467781 -0.07988507  0.07508503\n",
      "   0.05462148 -0.04141235  0.07145645  0.0308333  -0.00418036 -0.02700487\n",
      "   0.04760922 -0.01573735 -0.00513888  0.00906907 -0.06655    -0.02171599\n",
      "   0.01112987  0.02142444  0.05100318  0.04857115  0.06534775 -0.03718774\n",
      "  -0.06838916 -0.04144597  0.03955498 -0.03096634 -0.02567561 -0.02237871\n",
      "   0.02383919  0.06929962  0.04766508 -0.07485477  0.05784786  0.00258369\n",
      "   0.01695319 -0.07251924 -0.04364058  0.06489131  0.03917194 -0.01470128\n",
      "  -0.04921795 -0.06464823  0.01120196  0.01161872 -0.04710688 -0.05050945\n",
      "  -0.00403404  0.00268341 -0.00784444  0.04009788  0.05163324 -0.02946849\n",
      "  -0.00412007 -0.02314316  0.01673633 -0.02574484 -0.02365477  0.04674415\n",
      "  -0.06809828  0.0223749  -0.03630517 -0.02385872 -0.07183222 -0.02584982\n",
      "  -0.04323738  0.05943372 -0.05958354  0.06671643  0.02315641  0.00524793\n",
      "   0.05055672 -0.03955711  0.05902827  0.04143069  0.0492903   0.00546043\n",
      "  -0.07880909 -0.04929796  0.04978113 -0.01860635 -0.04680577  0.02326846\n",
      "  -0.02211912  0.04241602 -0.05195818  0.04204397  0.08039538  0.01669751\n",
      "   0.01431298  0.01247855  0.00818534 -0.03677214  0.02853741  0.03333968\n",
      "  -0.03557438 -0.06331643  0.02041701 -0.05382596 -0.07443587  0.03599976\n",
      "  -0.02287997 -0.02194352  0.02270694  0.00253991  0.02189825 -0.04916281\n",
      "  -0.03079832 -0.05649495 -0.01655334 -0.01198303 -0.05321699  0.01676976\n",
      "  -0.0673033  -0.0811203   0.07350467  0.06303971  0.02901043 -0.05026489\n",
      "  -0.07762023 -0.05059215]], shape=(1, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sentence_vector = universal_sentence_encoder([sentence])\n",
    "print(sentence_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard shape for any vector representation is 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512)\n"
     ]
    }
   ],
   "source": [
    "print(sentence_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute sentence similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. BioSentVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the sentence similarity between a sentence pair using the BioSentVec model. We firstly use the above code examples to get vector representations of sentences. Then we compute the cosine similarity between the pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity: 0.9813870787620544\n"
     ]
    }
   ],
   "source": [
    "sentence_vector1 = model.embed_sentence(preprocess_sentence('Breast cancers with HER2 amplification have a higher risk of CNS metastasis and poorer prognosis.'))\n",
    "sentence_vector2 = model.embed_sentence(preprocess_sentence('Breast cancers with HER2 amplification are more aggressive, have a higher risk of CNS metastasis, and poorer prognosis.'))\n",
    "\n",
    "cosine_sim = 1 - distance.cosine(sentence_vector1, sentence_vector2)\n",
    "print('cosine similarity:', cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another example for a pair that is relatively less similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity: 0.7300089001655579\n"
     ]
    }
   ],
   "source": [
    "sentence_vector3 = model.embed_sentence(preprocess_sentence('Furthermore, increased CREB expression in breast tumors is associated with poor prognosis, shorter survival and higher risk of metastasis.'))\n",
    "cosine_sim = 1 - distance.cosine(sentence_vector1, sentence_vector3)\n",
    "print('cosine similarity:', cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the sentence similarity between the same sentence pairs using the Universal Sentence Encoder model. Apply the model to each sentence, to get vector representations of the individual sentences, then compute the inner product of the two vectors to determine the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.9569721\n"
     ]
    }
   ],
   "source": [
    "sentence_vector1 = universal_sentence_encoder([preprocess_sentence('Breast cancers with HER2 amplification have a higher risk of CNS metastasis and poorer prognosis.')])\n",
    "sentence_vector2 = universal_sentence_encoder([preprocess_sentence('Breast cancers with HER2 amplification are more aggressive, have a higher risk of CNS metastasis, and poorer prognosis.')])\n",
    "sim = np.inner(sentence_vector1, sentence_vector2)[0][0]\n",
    "print('Similarity:', sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the less similar pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.5786929\n"
     ]
    }
   ],
   "source": [
    "sentence_vector3 = universal_sentence_encoder([preprocess_sentence('Furthermore, increased CREB expression in breast tumors is associated with poor prognosis, shorter survival and higher risk of metastasis.')])\n",
    "sim = np.inner(sentence_vector1, sentence_vector3)[0][0]\n",
    "print('Similarity:', sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "# Levenshtein distance\n",
    "def lev_dist(a, b):    \n",
    "    \n",
    "    @lru_cache(None)  # for memorization\n",
    "    def min_dist(s1, s2):\n",
    "\n",
    "        if s1 == len(a) or s2 == len(b):\n",
    "            return len(a) - s1 + len(b) - s2\n",
    "\n",
    "        # no change required\n",
    "        if a[s1] == b[s2]:\n",
    "            return min_dist(s1 + 1, s2 + 1)\n",
    "\n",
    "        return 1 + min(\n",
    "            min_dist(s1, s2 + 1),      # insert character\n",
    "            min_dist(s1 + 1, s2),      # delete character\n",
    "            min_dist(s1 + 1, s2 + 1),  # replace character\n",
    "        )\n",
    "\n",
    "    return min_dist(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. BIOSSES Test Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. BioSentVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioSentVec similarity score: 0.557557116150856\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "import numpy as np\n",
    "\n",
    "doc = docx.Document('../test_data/BIOSSES-Dataset/Annotation-Pairs.docx')\n",
    "tables = doc.tables\n",
    "data = []\n",
    "\n",
    "keys = None\n",
    "for i, row in enumerate(tables[0].rows):\n",
    "    text = (cell.text for cell in row.cells)\n",
    "\n",
    "    if i == 0:\n",
    "        keys = tuple(text)\n",
    "        continue\n",
    "\n",
    "    row_data = dict(zip(keys, text))\n",
    "    data.append(row_data)\n",
    "cosine_similarity_list = []\n",
    "for sentMap in data:\n",
    "    sentence_vector1 = model.embed_sentence(preprocess_sentence(sentMap['Sentence 1']))\n",
    "    sentence_vector2 = model.embed_sentence(preprocess_sentence(sentMap['Sentence 2']))\n",
    "    cosine_sim = 1 - distance.cosine(sentence_vector1, sentence_vector2)\n",
    "    cosine_similarity_list.append(cosine_sim)\n",
    "cosine_similarity_score = np.mean(cosine_similarity_list)\n",
    "print('BioSentVec similarity score:', cosine_similarity_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein similarity score: 0.31274520377736964\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "import numpy as np\n",
    "\n",
    "doc = docx.Document('../test_data/BIOSSES-Dataset/Annotation-Pairs.docx')\n",
    "tables = doc.tables\n",
    "data = []\n",
    "\n",
    "keys = None\n",
    "for i, row in enumerate(tables[0].rows):\n",
    "    text = (cell.text for cell in row.cells)\n",
    "\n",
    "    if i == 0:\n",
    "        keys = tuple(text)\n",
    "        continue\n",
    "\n",
    "    row_data = dict(zip(keys, text))\n",
    "    data.append(row_data)\n",
    "lev_dist_similarity_list = []\n",
    "for sentMap in data:\n",
    "    sentence_vector1 = preprocess_sentence(sentMap['Sentence 1'])\n",
    "    sentence_vector2 = preprocess_sentence(sentMap['Sentence 2'])\n",
    "    # Levenshtein distance\n",
    "    lev_sim = lev_dist(sentence_vector1, sentence_vector2)\n",
    "    lev_dist_similarity_list.append(lev_sim)\n",
    "min_distance = np.min(lev_dist_similarity_list)\n",
    "\n",
    "dist_list = []\n",
    "for dist in lev_dist_similarity_list:\n",
    "    dist_list.append(min_distance / dist)\n",
    "lev_similarity_score = np.mean(dist_list)\n",
    "print('Levenshtein similarity score:', lev_similarity_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Sentence Encoder similarity score: 0.36970225\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "import numpy as np\n",
    "\n",
    "doc = docx.Document('../test_data/BIOSSES-Dataset/Annotation-Pairs.docx')\n",
    "tables = doc.tables\n",
    "data = []\n",
    "\n",
    "keys = None\n",
    "for i, row in enumerate(tables[0].rows):\n",
    "    text = (cell.text for cell in row.cells)\n",
    "\n",
    "    if i == 0:\n",
    "        keys = tuple(text)\n",
    "        continue\n",
    "\n",
    "    row_data = dict(zip(keys, text))\n",
    "    data.append(row_data)\n",
    "sent_encoder_similarity_list = []\n",
    "for sentMap in data:\n",
    "    sentence_vector1 = universal_sentence_encoder([preprocess_sentence(sentMap['Sentence 1'])])\n",
    "    sentence_vector2 = universal_sentence_encoder([preprocess_sentence(sentMap['Sentence 2'])])\n",
    "    sim = np.inner(sentence_vector1, sentence_vector2)\n",
    "    sent_encoder_similarity_list.append(sim)\n",
    "similarity_score = np.mean(sent_encoder_similarity_list)\n",
    "print('Universal Sentence Encoder similarity score:', similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MayoSRS Test Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. BioSentVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioSentVec similarity score: 0.2514136122977909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naveentulseela/Library/Python/3.8/lib/python/site-packages/scipy/spatial/distance.py:630: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file = open('../test_data/MayoSRS.csv')\n",
    "csvreader = csv.reader(file)\n",
    "\n",
    "cosine_similarity_list = []\n",
    "\n",
    "for row in csvreader:\n",
    "    sentence_vector1 = model.embed_sentence(preprocess_sentence(row[3]))\n",
    "    sentence_vector2 = model.embed_sentence(preprocess_sentence(row[4]))\n",
    "    cosine_sim = 1 - distance.cosine(sentence_vector1, sentence_vector2)\n",
    "    cosine_similarity_list.append(cosine_sim)\n",
    "file.close()\n",
    "cosine_similarity_score = np.mean(cosine_similarity_list)\n",
    "print('BioSentVec similarity score:', cosine_similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein similarity score: 0.10480677938548977\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file = open('../test_data/MayoSRS.csv')\n",
    "csvreader = csv.reader(file)\n",
    "\n",
    "lev_dist_similarity_list = []\n",
    "for row in csvreader:\n",
    "    sentence_vector1 = preprocess_sentence(row[3])\n",
    "    sentence_vector2 = preprocess_sentence(row[4])\n",
    "    lev_sim = lev_dist(sentence_vector1, sentence_vector2)\n",
    "    lev_dist_similarity_list.append(lev_sim)\n",
    "min_distance = np.min(lev_dist_similarity_list)\n",
    "file.close()\n",
    "\n",
    "dist_list = []\n",
    "for dist in lev_dist_similarity_list:\n",
    "    dist_list.append(min_distance / dist)\n",
    "lev_similarity_score = np.mean(dist_list)\n",
    "print('Levenshtein similarity score:', lev_similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Sentence Encoder similarity score: 0.32283983\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file = open('../test_data/MayoSRS.csv')\n",
    "csvreader = csv.reader(file)\n",
    "sent_encoder_similarity_list = []\n",
    "\n",
    "for row in csvreader:\n",
    "    sentence_vector1 = universal_sentence_encoder([preprocess_sentence(row[3])])\n",
    "    sentence_vector2 = universal_sentence_encoder([preprocess_sentence(row[4])])\n",
    "    sim = np.inner(sentence_vector1, sentence_vector2)[0][0]\n",
    "    sent_encoder_similarity_list.append(sim)\n",
    "file.close()\n",
    "similarity_score = np.mean(sent_encoder_similarity_list)\n",
    "print('Universal Sentence Encoder similarity score:', similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. UMNSRS_similarity Test Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. BioSentVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioSentVec similarity score: 0.23443180937339755\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file = open('../test_data/UMNSRS_similarity.csv')\n",
    "csvreader = csv.reader(file)\n",
    "cosine_similarity_list = []\n",
    "\n",
    "for row in csvreader:\n",
    "    sentence_vector1 = model.embed_sentence(preprocess_sentence(row[2]))\n",
    "    sentence_vector2 = model.embed_sentence(preprocess_sentence(row[3]))\n",
    "    cosine_sim = 1 - distance.cosine(sentence_vector1, sentence_vector2)\n",
    "    cosine_similarity_list.append(cosine_sim)\n",
    "file.close()\n",
    "cosine_similarity_score = np.mean(cosine_similarity_list)\n",
    "print('BioSentVec similarity score:', cosine_similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein similarity score: 0.12853671720147514\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file = open('../test_data/UMNSRS_similarity.csv')\n",
    "csvreader = csv.reader(file)\n",
    "\n",
    "lev_dist_similarity_list = []\n",
    "for row in csvreader:\n",
    "    sentence_vector1 = preprocess_sentence(row[2])\n",
    "    sentence_vector2 = preprocess_sentence(row[3])\n",
    "    lev_sim = lev_dist(sentence_vector1, sentence_vector2)\n",
    "    if lev_sim != 0:\n",
    "        lev_dist_similarity_list.append(lev_sim)\n",
    "min_distance = np.min(lev_dist_similarity_list)\n",
    "\n",
    "dist_list = []\n",
    "for dist in lev_dist_similarity_list:\n",
    "    dist_list.append(min_distance / dist)\n",
    "lev_similarity_score = np.mean(dist_list)\n",
    "file.close()\n",
    "print('Levenshtein similarity score:', lev_similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Sentence Encoder similarity score: 0.3192265\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file = open('../test_data/UMNSRS_similarity.csv')\n",
    "csvreader = csv.reader(file)\n",
    "sent_encoder_similarity_list = []\n",
    "\n",
    "for row in csvreader:\n",
    "    sentence_vector1 = universal_sentence_encoder([preprocess_sentence(row[2])])\n",
    "    sentence_vector2 = universal_sentence_encoder([preprocess_sentence(row[3])])\n",
    "    sim = np.inner(sentence_vector1, sentence_vector2)[0][0]\n",
    "    sent_encoder_similarity_list.append(sim)\n",
    "file.close()\n",
    "similarity_score = np.mean(sent_encoder_similarity_list)\n",
    "print('Universal Sentence Encoder similarity score:', similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Comito, D. Falcone and A. Forestiero, \"AI-Driven Clinical Decision Support: Enhancing Disease Diagnosis Exploiting Patients Similarity,\" in IEEE Access, vol. 10, pp. 6878-6888, 2022, doi: 10.1109/ACCESS.2022.3142100.\n",
    "\n",
    "Chen Q, Peng Y, Lu Z. BioSentVec: creating sentence embeddings for biomedical texts. 2018. arXiv:1810.09302."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "6fd1cfc5a787f1ff6635659d3012a72d7cc12b3dd68a70b94b92aa3c71b7fa69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
